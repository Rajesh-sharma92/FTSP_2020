{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Pokeman.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPIqe4ojAaDn",
        "colab_type": "text"
      },
      "source": [
        "http://www.jessicayung.com/using-generators-in-python-to-train-machine-learning-models/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho0_Z2BfEEtU",
        "colab_type": "code",
        "outputId": "5c150842-4deb-48f6-87dd-66cea9fae702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqjGNkWdj7H",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7cgWj4Ifhjp",
        "colab_type": "code",
        "outputId": "3e4d1289-1ddc-4d05-dd87-075dad22a522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "print (os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2MrHUfOEKCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbKFBon9ESGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc9EDHuyd2Tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq_AO-WQEdv1",
        "colab_type": "code",
        "outputId": "b2a24115-1a19-4b9e-9666-a2f6ef3a4ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/dataset/test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4abf4cc288c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                  \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                  class_mode = 'categorical')\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m test_set = test_datagen.flow_from_directory('/content/drive/My Drive/dataset/test_set',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         )\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             dtype=dtype)\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/dataset/training_set'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhKgHYiFYxWE",
        "colab_type": "code",
        "outputId": "84699942-9219-48b8-9e5c-5d8db6efdcca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print (type(training_set))\n",
        "print (type(train_datagen))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'keras_preprocessing.image.directory_iterator.DirectoryIterator'>\n",
            "<class 'keras.preprocessing.image.ImageDataGenerator'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adRQ0gspM58u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs3z4qmgE1th",
        "colab_type": "code",
        "outputId": "060ab908-d56d-4820-dfee-dfc6449b47d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch = 100,\n",
        "                         epochs = 5,#25\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "  7/100 [=>............................] - ETA: 2:11 - loss: 1.0319 - acc: 0.4958"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:914: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 269s 3s/step - loss: 0.3668 - acc: 0.8468 - val_loss: 0.2321 - val_acc: 0.9077\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 254s 3s/step - loss: 0.0864 - acc: 0.9682 - val_loss: 0.2459 - val_acc: 0.9245\n",
            "Epoch 3/5\n",
            " 36/100 [=========>....................] - ETA: 50s - loss: 0.0743 - acc: 0.9721"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHAtDoM2P6Er",
        "colab_type": "text"
      },
      "source": [
        "## Using the pretrained model: VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uyazfWbMBbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_97RbbAjrLP5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljseih9Cxp9N",
        "colab_type": "code",
        "outputId": "cafcaded-5b77-4de3-fd10-1f4fea68d658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wwE9p4BOg0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdnEJ-c5I25J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cASZ4vNM8_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "import os\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1tbESS5ODt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False #do not train the conv-base"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urx8mGqnONmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/content/drive/My Drive/dataset/'\n",
        "train_dir = os.path.join(base_dir, 'training_set')\n",
        "\n",
        "test_dir = os.path.join(base_dir, 'test_set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnRMbUIcO0mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255) #rescale factor is feature scalling, convert values in the range of 0-1 before applying any processing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgGVkhW_PJMv",
        "colab_type": "code",
        "outputId": "3afd8418-de2c-4cbd-c27e-9bab5540255d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 445 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBQ5LhuzPdqL",
        "colab_type": "code",
        "outputId": "d315b1db-1a33-455c-8dee-8994ecb77381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  target_size=(150, 150),\n",
        "                                                  batch_size=20,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 238 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL18ENzzPyfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3HemyIVQEg2",
        "colab_type": "code",
        "outputId": "a2d30a3c-bc43-4425-a341-3adf1249ea0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1078
        }
      },
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=100,\n",
        "                              epochs=30,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "  4/100 [>.............................] - ETA: 2:26 - loss: 1.0805 - acc: 0.4000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:914: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
            "  'to RGBA images')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 101s 1s/step - loss: 0.2541 - acc: 0.8885 - val_loss: 0.0936 - val_acc: 0.9667\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 0.0694 - acc: 0.9800 - val_loss: 0.0311 - val_acc: 0.9879\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 92s 919ms/step - loss: 0.0219 - acc: 0.9940 - val_loss: 0.0427 - val_acc: 0.9909\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 93s 928ms/step - loss: 0.0210 - acc: 0.9915 - val_loss: 0.0500 - val_acc: 0.9960\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 91s 911ms/step - loss: 0.0186 - acc: 0.9965 - val_loss: 0.0576 - val_acc: 0.9950\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 92s 921ms/step - loss: 0.0123 - acc: 0.9970 - val_loss: 0.0366 - val_acc: 0.9960\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 92s 921ms/step - loss: 0.0069 - acc: 0.9965 - val_loss: 0.0259 - val_acc: 0.9960\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 92s 915ms/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.0292 - val_acc: 0.9960\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 92s 920ms/step - loss: 0.0132 - acc: 0.9975 - val_loss: 0.0842 - val_acc: 0.9950\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 93s 935ms/step - loss: 0.0126 - acc: 0.9980 - val_loss: 0.0309 - val_acc: 0.9960\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 91s 908ms/step - loss: 0.0113 - acc: 0.9990 - val_loss: 0.0651 - val_acc: 0.9960\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 0.0072 - acc: 0.9990 - val_loss: 0.0851 - val_acc: 0.9838\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 91s 907ms/step - loss: 2.3728e-05 - acc: 1.0000 - val_loss: 0.0758 - val_acc: 0.9879\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 94s 937ms/step - loss: 0.0068 - acc: 0.9985 - val_loss: 0.0514 - val_acc: 0.9960\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 92s 917ms/step - loss: 0.0010 - acc: 0.9995 - val_loss: 0.0911 - val_acc: 0.9829\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 92s 915ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0740 - val_acc: 0.9869\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 92s 918ms/step - loss: 0.0071 - acc: 0.9990 - val_loss: 0.0681 - val_acc: 0.9909\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 8.6247e-05 - acc: 1.0000 - val_loss: 0.1449 - val_acc: 0.9798\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 91s 908ms/step - loss: 0.0059 - acc: 0.9995 - val_loss: 0.0831 - val_acc: 0.9788\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 91s 906ms/step - loss: 1.2475e-06 - acc: 1.0000 - val_loss: 0.0658 - val_acc: 0.9960\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 93s 934ms/step - loss: 2.4079e-07 - acc: 1.0000 - val_loss: 0.0665 - val_acc: 0.9960\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 90s 905ms/step - loss: 1.3540e-07 - acc: 1.0000 - val_loss: 0.1211 - val_acc: 0.9829\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 92s 921ms/step - loss: 0.0145 - acc: 0.9975 - val_loss: 0.0715 - val_acc: 0.9960\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 92s 922ms/step - loss: 8.6491e-06 - acc: 1.0000 - val_loss: 0.0741 - val_acc: 0.9879\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 90s 904ms/step - loss: 0.0328 - acc: 0.9970 - val_loss: 0.1189 - val_acc: 0.9748\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 91s 912ms/step - loss: 2.1321e-04 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9950\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 93s 926ms/step - loss: 1.6873e-07 - acc: 1.0000 - val_loss: 0.0663 - val_acc: 0.9960\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 89s 895ms/step - loss: 0.0125 - acc: 0.9990 - val_loss: 0.1260 - val_acc: 0.9879\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 92s 916ms/step - loss: 2.2173e-06 - acc: 1.0000 - val_loss: 0.1013 - val_acc: 0.9778\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 89s 890ms/step - loss: 0.0408 - acc: 0.9970 - val_loss: 0.0764 - val_acc: 0.9838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR6r9YZ2tuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkGu61Rhe14w",
        "colab_type": "text"
      },
      "source": [
        "### Using a pre trained model: Fine Tuning the pre trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuv6G51tqh8n",
        "colab_type": "text"
      },
      "source": [
        "Another widely used technique for model reuse, complementary to feature\n",
        "extraction, is fine-tuning (see figure 5.19). Fine-tuning consists of unfreezing a few of\n",
        "the top layers of a frozen model base used for feature extraction, and jointly training\n",
        "both the newly added part of the model (in this case, the fully connected classifier)\n",
        "and these top layers. This is called fine-tuning because it slightly adjusts the more\n",
        "abstract representations of the model being reused, in order to make them more relevant\n",
        "for the problem at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2f19cAwq_dC",
        "colab_type": "text"
      },
      "source": [
        "Thus the\n",
        "steps for fine-tuning a network are as follow:\n",
        "1 Add your custom network on top of an already-trained base network.\n",
        "2 Freeze the base network.\n",
        "3 Train the part you added.\n",
        "4 Unfreeze some layers in the base network.\n",
        "5 Jointly train both these layers and the part you added.\n",
        "You already completed the first three steps when doing feature extraction. Let’s proceed\n",
        "with step 4: you’ll unfreeze your conv_base and then freeze individual layers\n",
        "inside it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n9D3Z2VrRim",
        "colab_type": "text"
      },
      "source": [
        "You’ll fine-tune the last three convolutional layers, which means all layers up to\n",
        "block4_pool should be frozen, and the layers block5_conv1, block5_conv2, and\n",
        "block5_conv3 should be trainable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zsdi8b8rSKq",
        "colab_type": "code",
        "outputId": "56acc092-16af-4827-8df0-5ce8fc58edb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "    \n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-025c4db58993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mset_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'block5_conv1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mset_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conv_base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fMV9hwde79c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NdwrCgEsbCz",
        "colab_type": "text"
      },
      "source": [
        "Here’s what you should take away from the exercises in the past two sections:\n",
        "\n",
        "Convnets are the best type of machine-learning models for computer-vision\n",
        "tasks. It’s possible to train one from scratch even on a very small dataset, with\n",
        "decent results.\n",
        "\n",
        "On a small dataset, overfitting will be the main issue. Data augmentation is a\n",
        "powerful way to fight overfitting when you’re working with image data.\n",
        "\n",
        "It’s easy to reuse an existing convnet on a new dataset via feature extraction.\n",
        "This is a valuable technique for working with small image datasets.\n",
        "\n",
        "As a complement to feature extraction, you can use fine-tuning, which adapts to\n",
        "a new problem some of the representations previously learned by an existing\n",
        "model. This pushes performance a bit further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huvEVNvcsaHf",
        "colab_type": "code",
        "outputId": "9db72f30-def0-49fb-9c5c-b9cbf56de867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "divmod(23,5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr2n40Sg4_p5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}